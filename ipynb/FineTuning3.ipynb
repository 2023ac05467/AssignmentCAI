{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8008e64f",
   "metadata": {},
   "source": [
    "### Group 100 -  Conversational AI Assignment 2\n",
    "\n",
    "\n",
    "```\n",
    "1. Amit Kumar Sharma      2023ac05454       100%\n",
    "\n",
    "2. Mohammed Faisal Sait   2023aa05525       100%\n",
    "\n",
    "3. Chachiya Faiz Arif     2023ac05420       100%\n",
    "\n",
    "4. Parveen Kumar          2023ac05467       100%\n",
    "\n",
    "5. Sachchinda Nand Singh  2023ac05002       100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a27e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (4.55.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: peft in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from peft) (2.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/llmenv/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets peft accelerate evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46bc16",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "## importing libraries\n",
    "# ============================================\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments, Trainer, pipeline,\n",
    "    DataCollatorForSeq2Seq,EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935bc62",
   "metadata": {},
   "source": [
    "### Generate df from Q&A csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86553d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined dataset with 98 samples.\n",
      "                                            Question         Answer\n",
      "0              What was SAP's total revenue in 2023?  €31.2 billion\n",
      "1              What was SAP's cloud revenue in 2023?  €13.6 billion\n",
      "2  What was SAP's software licenses revenue in 2023?   €2.7 billion\n",
      "3           What was SAP's services revenue in 2023?   €4.3 billion\n",
      "4           What was SAP's operating profit in 2023?   €5.8 billion\n"
     ]
    }
   ],
   "source": [
    "# List of CSV files\n",
    "csv_paths = [\n",
    "    \"../data/Q&A/qa_dataset.csv\",\n",
    "    \"../data/Q&A/new_financial_qa.csv\"\n",
    "]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df_list = []\n",
    "for path in csv_paths:\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df_list.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File not found: {path}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Loaded combined dataset with {len(df)} samples.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5ac9b",
   "metadata": {},
   "source": [
    "### Create dataset for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95dc151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acd61b",
   "metadata": {},
   "source": [
    "### Generate Train and Test dataset 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7308d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad51722",
   "metadata": {},
   "source": [
    "### 3.2 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Tokenizer & Model Selection\n",
    "# ============================================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],  # for T5\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07912f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/78 [00:00<?, ? examples/s]/opt/anaconda3/envs/llmenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 718.05 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 1594.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Preprocessing function (mask pad tokens)\n",
    "# ============================================\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"question: \" + q.strip() for q in examples[\"Question\"]]\n",
    "    targets = [a.strip() for a in examples[\"Answer\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).input_ids\n",
    "\n",
    "    # Mask pad tokens in labels\n",
    "    labels = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Data collator\n",
    "# =======================================\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2bb78",
   "metadata": {},
   "source": [
    "### 3.3 Baseline Benchmarking (Pre-Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60bc9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Evaluation...\n",
      "\n",
      "Average Accuracy: 0.0\n",
      "                                            Question  \\\n",
      "0  What is SAP’s capital stock as of December 31,...   \n",
      "1          What is SAP Business Technology Platform?   \n",
      "2  What percentage of revenue came from the Ameri...   \n",
      "3       How many customers does SAP serve worldwide?   \n",
      "4  How much does SAP plan to invest in AI over th...   \n",
      "5  What percentage of the world's GDP touches SAP...   \n",
      "6              How much was support revenue in 2023?   \n",
      "7                            What is SAP Datasphere?   \n",
      "8                 When did Christian Klein join SAP?   \n",
      "9        What is SAP's Intelligent Spend Management?   \n",
      "\n",
      "                                         True Answer  \\\n",
      "0  SAP’s capital stock as of December 31, 2023, w...   \n",
      "1     A platform integrating data, AI, and analytics   \n",
      "2  The Americas contributed 41% to total revenue ...   \n",
      "3                             Over 400,000 customers   \n",
      "4                                    Over €1 billion   \n",
      "5                                                87%   \n",
      "6        Support revenue was €6,787 million in 2023.   \n",
      "7                          A data management service   \n",
      "8                Christian Klein joined SAP in 1999.   \n",
      "9    A solution to optimize procurement and expenses   \n",
      "\n",
      "                     Predicted Answer  Accuracy  Confidence  \\\n",
      "0                             900,000         0      0.1526   \n",
      "1    SAP Business Technology Platform         0      0.4805   \n",
      "2                                  5%         0      0.0975   \n",
      "3                             500,000         0      0.1303   \n",
      "4                        $1.2 billion         0      0.1366   \n",
      "5                         1.2 billion         0      0.1779   \n",
      "6                        $1.1 billion         0      0.1243   \n",
      "7                      SAP Datasphere         0      0.4764   \n",
      "8                                2007         0      0.0391   \n",
      "9  SAP's Intelligent Spend Management         0      0.0683   \n",
      "\n",
      "   Inference Time (s)  \n",
      "0               0.488  \n",
      "1               0.574  \n",
      "2               0.122  \n",
      "3               0.126  \n",
      "4               0.148  \n",
      "5               0.151  \n",
      "6               0.187  \n",
      "7               0.718  \n",
      "8               0.159  \n",
      "9               0.224  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Benchmarking function\n",
    "# ========================================\n",
    "def benchmark_model(model, tokenizer, test_df, n=10):\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "    logs = []\n",
    "    correct = 0\n",
    "\n",
    "    for _, row in test_df.head(n).iterrows():\n",
    "        q = row[\"Question\"]\n",
    "        a_true = str(row[\"Answer\"]).strip()\n",
    "\n",
    "        prompt = \"question: \" + q\n",
    "\n",
    "        start_time = time.time()\n",
    "        output = pipe(prompt, max_new_tokens=32, do_sample=False)\n",
    "        end_time = time.time()\n",
    "\n",
    "        a_pred = output[0]['generated_text'].strip()\n",
    "\n",
    "        # Confidence proxy\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=32,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        conf = None\n",
    "        if outputs.scores:\n",
    "            probs = torch.nn.functional.softmax(outputs.scores[0][0], dim=-1)\n",
    "            conf = probs.max().item()\n",
    "\n",
    "        # Manual accuracy: case-insensitive substring match\n",
    "        acc = 1 if a_true.lower() in a_pred.lower() else 0\n",
    "        correct += acc\n",
    "\n",
    "        logs.append({\n",
    "            \"Question\": q,\n",
    "            \"True Answer\": a_true,\n",
    "            \"Predicted Answer\": a_pred,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Confidence\": round(conf, 4) if conf else None,\n",
    "            \"Inference Time (s)\": round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    avg_accuracy = correct / n\n",
    "    df_log = pd.DataFrame(logs)\n",
    "    print(\"\\nAverage Accuracy:\", round(avg_accuracy, 4))\n",
    "    return df_log\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Baseline before finetuning\n",
    "# ============================================\n",
    "print(\"Baseline Model Evaluation...\")\n",
    "baseline_results = benchmark_model(model, tokenizer, test_dataset.to_pandas(), n=10)\n",
    "print(baseline_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd485a06",
   "metadata": {},
   "source": [
    "### 3.4 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70681dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yj/6wjp86_950d283vn72mxzmy40000gn/T/ipykernel_43083/598176017.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 12:54, Epoch 80/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.778700</td>\n",
       "      <td>3.182318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.689300</td>\n",
       "      <td>3.124519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.879800</td>\n",
       "      <td>3.051590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.573300</td>\n",
       "      <td>2.971312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.318800</td>\n",
       "      <td>2.895342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.239500</td>\n",
       "      <td>2.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.216500</td>\n",
       "      <td>2.799684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>2.772446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.477700</td>\n",
       "      <td>2.749204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.604300</td>\n",
       "      <td>2.732127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.247400</td>\n",
       "      <td>2.712289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.769600</td>\n",
       "      <td>2.695939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.104700</td>\n",
       "      <td>2.676786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.281500</td>\n",
       "      <td>2.657475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.870100</td>\n",
       "      <td>2.640205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.995800</td>\n",
       "      <td>2.623524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.138800</td>\n",
       "      <td>2.611191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.973600</td>\n",
       "      <td>2.597929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.298900</td>\n",
       "      <td>2.584777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.924200</td>\n",
       "      <td>2.566357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.257700</td>\n",
       "      <td>2.553109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.988100</td>\n",
       "      <td>2.538923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.785900</td>\n",
       "      <td>2.526057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.002800</td>\n",
       "      <td>2.511824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.732700</td>\n",
       "      <td>2.498608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.809600</td>\n",
       "      <td>2.491298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.025300</td>\n",
       "      <td>2.483243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.024600</td>\n",
       "      <td>2.470957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.105100</td>\n",
       "      <td>2.462229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.880400</td>\n",
       "      <td>2.452729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.081700</td>\n",
       "      <td>2.444767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.737900</td>\n",
       "      <td>2.437988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.675300</td>\n",
       "      <td>2.432351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.711400</td>\n",
       "      <td>2.428290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.578500</td>\n",
       "      <td>2.418988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.947000</td>\n",
       "      <td>2.415745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.597500</td>\n",
       "      <td>2.409346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.798400</td>\n",
       "      <td>2.403208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.871200</td>\n",
       "      <td>2.398426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.012200</td>\n",
       "      <td>2.395651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.746000</td>\n",
       "      <td>2.391092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.458600</td>\n",
       "      <td>2.385833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.027100</td>\n",
       "      <td>2.384127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.083600</td>\n",
       "      <td>2.381248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.177700</td>\n",
       "      <td>2.378494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.736800</td>\n",
       "      <td>2.376583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.847500</td>\n",
       "      <td>2.374375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.737700</td>\n",
       "      <td>2.370360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.417200</td>\n",
       "      <td>2.366206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.752000</td>\n",
       "      <td>2.363490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.924300</td>\n",
       "      <td>2.361622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.865800</td>\n",
       "      <td>2.358643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.116100</td>\n",
       "      <td>2.354926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.689000</td>\n",
       "      <td>2.352776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.574900</td>\n",
       "      <td>2.351221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.535200</td>\n",
       "      <td>2.350424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.769000</td>\n",
       "      <td>2.348038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.423700</td>\n",
       "      <td>2.346636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.593200</td>\n",
       "      <td>2.345135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.686600</td>\n",
       "      <td>2.344802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.708400</td>\n",
       "      <td>2.344050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.892900</td>\n",
       "      <td>2.341323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.759100</td>\n",
       "      <td>2.340231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.660900</td>\n",
       "      <td>2.340148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.900300</td>\n",
       "      <td>2.339155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.887100</td>\n",
       "      <td>2.338126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.059700</td>\n",
       "      <td>2.337388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.481000</td>\n",
       "      <td>2.336488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.676000</td>\n",
       "      <td>2.336123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.743400</td>\n",
       "      <td>2.335389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.883900</td>\n",
       "      <td>2.335023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.309400</td>\n",
       "      <td>2.334520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.633800</td>\n",
       "      <td>2.334384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.436600</td>\n",
       "      <td>2.334298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.663800</td>\n",
       "      <td>2.334103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.503300</td>\n",
       "      <td>2.333992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.419100</td>\n",
       "      <td>2.333836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.481500</td>\n",
       "      <td>2.333712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.537800</td>\n",
       "      <td>2.333653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.464900</td>\n",
       "      <td>2.333631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1600, training_loss=2.8769822406768797, metrics={'train_runtime': 781.8814, 'train_samples_per_second': 7.981, 'train_steps_per_second': 2.046, 'total_flos': 293286916915200.0, 'train_loss': 2.8769822406768797, 'epoch': 80.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,          # Because lower loss is better\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=80,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d516e7f",
   "metadata": {},
   "source": [
    "### Benchmarking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abd724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Finetuning Model Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmenv/lib/python3.11/site-packages/transformers/pytorch_utils.py:333: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.00%\n",
      "Average Inference Time: 3.709s\n",
      "                                            Question  \\\n",
      "0  What is SAP’s capital stock as of December 31,...   \n",
      "1          What is SAP Business Technology Platform?   \n",
      "2  What percentage of revenue came from the Ameri...   \n",
      "3       How many customers does SAP serve worldwide?   \n",
      "4  How much does SAP plan to invest in AI over th...   \n",
      "5  What percentage of the world's GDP touches SAP...   \n",
      "6              How much was support revenue in 2023?   \n",
      "7                            What is SAP Datasphere?   \n",
      "8                 When did Christian Klein join SAP?   \n",
      "9        What is SAP's Intelligent Spend Management?   \n",
      "\n",
      "                                         True Answer  \\\n",
      "0  SAP’s capital stock as of December 31, 2023, w...   \n",
      "1     A platform integrating data, AI, and analytics   \n",
      "2  The Americas contributed 41% to total revenue ...   \n",
      "3                             Over 400,000 customers   \n",
      "4                                    Over €1 billion   \n",
      "5                                                87%   \n",
      "6        Support revenue was €6,787 million in 2023.   \n",
      "7                          A data management service   \n",
      "8                Christian Klein joined SAP in 1999.   \n",
      "9    A solution to optimize procurement and expenses   \n",
      "\n",
      "                                    Predicted Answer  Match  \\\n",
      "0  SAP’s capital stock as of December 31, 2023 is...  False   \n",
      "1                   SAP Business Technology Platform  False   \n",
      "2                 US revenue came from the Americas.  False   \n",
      "3  SAP serves a total of 350,000 customers worldw...  False   \n",
      "4        SAP plans to invest in AI over three years.  False   \n",
      "5                         GDP is 2.8 billion a year.  False   \n",
      "6         Support revenue in 2023 was €13.6 billion.  False   \n",
      "7  SAP Datasphere is a data analytics software pl...  False   \n",
      "8        Christian Klein joined SAP in January 2015.  False   \n",
      "9                       Intelligent Spend Management  False   \n",
      "\n",
      "   Inference Time (s)  \n",
      "0              18.392  \n",
      "1               2.468  \n",
      "2               1.749  \n",
      "3               1.828  \n",
      "4               3.877  \n",
      "5               1.837  \n",
      "6               2.002  \n",
      "7               2.238  \n",
      "8               1.066  \n",
      "9               1.630  \n"
     ]
    }
   ],
   "source": [
    "def benchmark_model(model, tokenizer, test_df, n=10):\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "    logs = []\n",
    "    correct = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for i, row in test_df.head(n).iterrows():\n",
    "        q = row[\"Question\"]\n",
    "        a_true = str(row[\"Answer\"]).strip()\n",
    "\n",
    "        start_time = time.time()\n",
    "        output = pipe(q, max_length=256, num_return_sequences=1)  # removed return_full_text\n",
    "        end_time = time.time()\n",
    "\n",
    "        a_pred = output[0]['generated_text'].strip()\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "        # Normalize for comparison\n",
    "        norm_true = a_true.lower().replace(\",\", \"\")\n",
    "        norm_pred = a_pred.lower().replace(\",\", \"\")\n",
    "\n",
    "        acc = 1 if norm_true in norm_pred else 0\n",
    "        correct += acc\n",
    "\n",
    "        logs.append({\n",
    "            \"Question\": q,\n",
    "            \"True Answer\": a_true,\n",
    "            \"Predicted Answer\": a_pred,\n",
    "            \"Match\": bool(acc),\n",
    "            \"Inference Time (s)\": round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    accuracy = correct / n\n",
    "    avg_time = total_time / n\n",
    "\n",
    "    print(f\"Average Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Average Inference Time: {avg_time:.3f}s\")\n",
    "\n",
    "    return pd.DataFrame(logs)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Post-finetuning evaluation\n",
    "# ============================================\n",
    "print(\"Post-Finetuning Model Evaluation...\")\n",
    "finetuned_results = benchmark_model(model, tokenizer, test_dataset.to_pandas(), n=10)\n",
    "print(finetuned_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb313e8",
   "metadata": {},
   "source": [
    "### Save FT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c113312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qa_finetuned_model_saved/tokenizer_config.json',\n",
       " './qa_finetuned_model_saved/special_tokens_map.json',\n",
       " './qa_finetuned_model_saved/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "### Save fine-tuned adapter + tokenizer\n",
    "# ============================================\n",
    "save_path = \"./qa_finetuned_model_saved\"\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save full model + tokenizer\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a53cd",
   "metadata": {},
   "source": [
    "### Test fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3334c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue in 2023 was €12.8 billion.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "save_path = \"./qa_finetuned_model_saved\"\n",
    "\n",
    "# Reload merged model\n",
    "model_ft = AutoModelForSeq2SeqLM.from_pretrained(save_path)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "qa_pipeline_ft = pipeline(\"text2text-generation\", model=model_ft, tokenizer=tokenizer_ft, device=-1)\n",
    "\n",
    "question = \"What was SAP's total revenue in 2023?\"\n",
    "prompt = \"question: \" + question\n",
    "print(qa_pipeline_ft(prompt, max_new_tokens=32, do_sample=False)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51854e33",
   "metadata": {},
   "source": [
    "### Testing FT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f08f777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'net profit'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "tok = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "m = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", low_cpu_mem_usage=True)\n",
    "pipe = pipeline(\"text2text-generation\", model=m, tokenizer=tok, device=-1)\n",
    "print(pipe(\"What was SAP's cloud revenue in 2023?\", max_new_tokens=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f937cc",
   "metadata": {},
   "source": [
    "## Testing FT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edd59294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"SAP's cloud revenue in 2024 accounted for 36% of its cloud revenue in 2024.\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "path = \"qa_finetuned_model_saved\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "# Try loading with conservative settings\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=\"auto\",            # let HF pick; change to torch.float32 if needed\n",
    "    low_cpu_mem_usage=True,        # reduces memory peak on CPU\n",
    "    device_map=None                # ensure CPU\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "print(pipe(\"What was SAP's cloud revenue in 2024?\", max_new_tokens=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b363a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded successfully.\n",
      "[{'generated_text': \"SAP's cloud revenue in 2023 was €13.6 billion.\"}]\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_path = \"qa_finetuned_model_saved\"\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "model_ft = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    finetuned_model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=None\n",
    ")\n",
    "qa_pipeline_ft = pipeline(\"text2text-generation\", model=model_ft, tokenizer=tokenizer_ft, device=-1)\n",
    "print(\"Fine-tuned model loaded successfully.\")\n",
    "print(qa_pipeline_ft(\"What was SAP's cloud revenue in 2023?\", max_new_tokens=64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
